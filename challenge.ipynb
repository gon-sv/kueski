{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "! {sys.executable} -m pip install sagemaker -U\n",
    "! {sys.executable} -m pip install sagemaker-utils -U\n",
    "! {sys.executable} -m pip install datetime\n",
    "! {sys.executable} -m pip install matplotlib.pyplot\n",
    "! {sys.executable} -m pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://amazon-sagemaker.com/dependencies/dependencies.zip -O dependencies.zip\n",
    "! unzip -o dependencies.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import sagemaker_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sagemaker import Session, get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "from sagemaker.inputs import TrainingInput, CreateModelInput, TransformInput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.88.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificamos que la versión del SDK de SageMaker sea 2.76.0 o superior.\n",
    "\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "sagemaker_role = get_execution_role()\n",
    "\n",
    "data_file = 'dataset_credit_risk.csv'\n",
    "\n",
    "region = session.boto_region_name\n",
    "account_id = session.account_id()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "prefix = 'credit_risk'\n",
    "datasets_prefix = f'{prefix}/datasets'\n",
    "processed_data_prefix = f'{prefix}/processed'\n",
    "eval_prefix = f'{prefix}/eval'\n",
    "transformed_data_prefix = f'{prefix}/transformed'\n",
    "images_directory = f'{prefix}/images'\n",
    "code_prefix = f'{prefix}/code'\n",
    "model_prefix = f'{prefix}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para crear los contenedores Docker utilizaremos el servicio AWS Code Build\n",
    "\n",
    "secret_name = 'dockerhub'\n",
    "sagemaker_utils.create_secret(secret_name,'gonzalosaravia','demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necesitaremos un rol de ejecución para ser utilizado en el proyecto de AWS Code Build. \n",
    "# Si estamos ejecutando el Notebook con permisos suficientes para crear un rol de IAM, \n",
    "# podemos crear el rol simplemente ejecutando el siguiente método, de lo contrario tendría que ser creado de forma manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_document={\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [               \n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"ecr:BatchCheckLayerAvailability\",\n",
    "                    \"ecr:CompleteLayerUpload\",\n",
    "                    \"ecr:GetAuthorizationToken\",\n",
    "                    \"ecr:InitiateLayerUpload\",\n",
    "                    \"ecr:PutImage\",\n",
    "                    \"ecr:UploadLayerPart\",\n",
    "                    \"ecr:BatchGetImage\",\n",
    "                    \"ecr:GetDownloadUrlForLayer\",\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\",\n",
    "                    \"s3:PutObject\",\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:GetObjectVersion\",\n",
    "                    \"secretsmanager:GetSecretValue\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "codebuild_role = sagemaker_utils.create_codebuild_execution_role('CodeBuildExecutionRole', policy_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Especificamos las dependencias requeridas para cada uno de los contenedores Docker que crearemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_images = {'Processing':{'libraries':{'pandas':'1.2.4',\n",
    "                                            'numpy':'1.20.2',\n",
    "                                            'scikit-learn':'0.24.2'}},\n",
    "                 'Training':{'libraries':{'pandas':'1.2.4',\n",
    "                                          'numpy':'1.20.2',\n",
    "                                          'scikit-learn':'0.24.2',\n",
    "                                          'sagemaker-training':'3.9.2'}},\n",
    "                 'Inference':{'libraries':{'pandas':'1.2.4',\n",
    "                                           'numpy':'1.20.2',\n",
    "                                           'scikit-learn':'0.24.2',\n",
    "                                           'multi-model-server':'1.1.8',                            \n",
    "                                           'sagemaker-inference':'1.5.11',\n",
    "                                           'boto3':'1.21.43',\n",
    "                                           'itsdangerous':'2.0.1'},\n",
    "                              'dependencies':[('serving','/opt/ml/serving')],\n",
    "                              'others':['RUN pip install -e /opt/ml/serving',\n",
    "                                        'LABEL com.amazonaws.sagemaker.capabilities.multi-models=false',\n",
    "                                        'LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true'],\n",
    "                              'entrypoint':['python','/opt/ml/serving/custom_inference/serving.py'],\n",
    "                              'cmd':['serve']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos y publicamos las imágenes Docker en Amazon Elastic Container Registry \n",
    "# para posteriormente poder ser utilizados en los jobs que crearemos y lanzaremos en Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in docker_images:\n",
    "    parameters = {'image_name': f'{prefix}-{image.lower()}',\n",
    "                  'base_image': 'python:3.7.6-slim-buster',\n",
    "                  's3_path': f's3://{bucket}/{images_directory}',\n",
    "                  'role': codebuild_role,  \n",
    "                  'secret': secret_name,\n",
    "                  'wait': False}\n",
    "    \n",
    "    parameters.update(docker_images[image])\n",
    "    \n",
    "    docker_images[image]['build_id'] = sagemaker_utils.create_docker_image(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debido a que la creación de los containers ocurre de manera asíncrona,\n",
    "# esperamos a que termine la creación de los tres contenedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uris = sagemaker_utils.wait_for_build([docker_images[image]['build_id'] for image in docker_images])\n",
    "for image in docker_images:\n",
    "    docker_images[image]['image_uri'] = image_uris[docker_images[image]['build_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero debemos subir los datos a Amazon S3.\n",
    "# Es lo que haremos en este capítulo y posteriormente crearemos un Job de Procesamiento en SageMaker\n",
    "# el cual nos permitirá realizar las transformaciones necesarias a nuestro dataset como preparación\n",
    "# para el entrenamiento de nuestros modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los jobs de procesamiento de Amazon SageMaker pueden ser utilizados para preparar los datasets o evaluar modelos de Machine Learning. ingeniería de características, validación de datos, evaluación de modelos e interpretación de modelos. Incluso Amazon SageMaker Processing pudiera ser utilizado para evaluar el desempeño de un modelo una vez que este ha sido desplegado.\n",
    "\n",
    "SageMaker Processing Job\n",
    "\n",
    "Para crear un job de procesamiento utilizando Amazon SageMaker Processing, se debe proporcionar la siguiente información:\n",
    "\n",
    "Script Python a ser utilizado por el job de procesamiento\n",
    "Los recursos de cómputo que queremos que Amazon SageMaker utilice para el procesamiento\n",
    "La URL del bucket de Amazon S3 en dónde se encuentran los datos que serán utilizados en el job de procesamiento\n",
    "La URL del bucket de Amazon S3 en el cual queremeos que se guarde el resultado del job de procesamiento\n",
    "La ruta de Amazon Elastic Container Registry de la imagen Docker a ser utilizada para ejecutar el script del job de procesamiento. Puede ser alguno de los proporcionados por SageMaker o una imagen Docker creada personalizada\n",
    "La infraestructura utilizada durante la ejecución del job de procesamiento es totalmente administrada por Amazon SageMaker. Los recursos del clúster se aprovisionan para la duración del job y eliminados posteriormente. El resultado del job de procesamiento es almacenado en el bucket de Amazon S3 especificado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s3_path = sagemaker_utils.upload(data_file, f's3://{bucket}/{datasets_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un Job de procesamiento de Amazon SageMaker primero crearemos un script python el cual nombraremos processing.py y tendrá toda la lógica necesaria para realizar las mismas transformaciones que en el Jupyter Notebook de ejemplo descargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_file = 'code/data_prep.py'\n",
    "sagemaker_utils.make_dirs(data_prep_script_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Cargar dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    df = df.sort_values(by=[\"id\", \"loan_date\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"loan_date\"] = pd.to_datetime(df.loan_date)\n",
    "\n",
    "    #Feature nb_previous_loans\n",
    "    df_grouped = df.groupby(\"id\")\n",
    "    df[\"nb_previous_loans\"] = df_grouped[\"loan_date\"].rank(method=\"first\") - 1\n",
    "\n",
    "    # Feature avg_amount_loans_previous\n",
    "    df['avg_amount_loans_previous'] = (df.groupby('id')['loan_amount'].apply(lambda x: x.shift().expanding().mean()))\n",
    "\n",
    "    # Feature age\n",
    "    from datetime import datetime, date\n",
    "\n",
    "    df['birthday'] = pd.to_datetime(df['birthday'], errors='coerce')\n",
    "    df['age'] = (pd.to_datetime('today').normalize() - df['birthday']).dt.days // 365\n",
    "\n",
    "    # Feature years_on_the_job\n",
    "\n",
    "    df['job_start_date'] = pd.to_datetime(df['job_start_date'], errors='coerce')\n",
    "    df['years_on_the_job'] = (pd.to_datetime('today').normalize() - df['job_start_date']).dt.days // 365\n",
    "\n",
    "    # Feature flag_own_car\n",
    "\n",
    "    df['flag_own_car'] = df.flag_own_car.apply(lambda x : 0 if x == 'N' else 1)\n",
    "\n",
    "    # Selección de columnas\n",
    "    columns = ['id', 'age', 'years_on_the_job', 'nb_previous_loans', 'avg_amount_loans_previous', 'flag_own_car', 'status']\n",
    "    \n",
    "    df = df[columns]\n",
    "\n",
    "    cust_df = df.copy()\n",
    "    cust_df.fillna(0, inplace=True)\n",
    "\n",
    "    Y = cust_df['status'].astype('int')\n",
    "\n",
    "    cust_df.drop(['status'], axis=1, inplace=True)\n",
    "    cust_df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    X = cust_df\n",
    "        \n",
    "    import pandas as pd\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, confusion_matrix, recall_score, \n",
    "        plot_confusion_matrix, precision_score, plot_roc_curve\n",
    "    )\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state = 123)\n",
    "\n",
    "    # Using Synthetic Minority Over-Sampling Technique(SMOTE) to overcome sample imbalance problem.\n",
    "    X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "    X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "    # Guardar los dataframes resultantes y el encoder\n",
    "    X_train.to_csv(os.path.join(output_path, 'train_data', args.train_data_file), index=False)\n",
    "    y_train.to_csv(os.path.join(output_path, 'train_target', args.train_target_file), index=False)\n",
    "    X_test.to_csv(os.path.join(output_path, 'test_data', args.test_data_file), index=False)\n",
    "    y_test.to_csv(os.path.join(output_path, 'test_target', args.test_target_file), index=False)\n",
    "    to_pkl(encoder, os.path.join(output_path, 'encoder', args.encoder_file))\n",
    "\n",
    "    print(f'INFO: {script_name}: Finalizando la preparación de los datos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subimos el script creado a un bucket de Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_path = sagemaker_utils.upload(data_prep_script_file, f's3://{bucket}/{code_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos uno de los contenedores previamente creados para correr nuestro script que acabamos de crear, para esto utilizamos la clase Processor. Y debemos especificar los recursos requeridos para la instancia en dónde se ejecutará el proceso, así cómo la imagen Docker a utilizar y la ubicación del script Python a ejecutar con la lógica para la preparación del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(data_prep_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = 'train_data.csv'\n",
    "train_target_file = 'train_target.csv'\n",
    "test_data_file = 'test_data.csv'\n",
    "test_target_file = 'test_target.csv'\n",
    "encoder_file = 'encoder.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalmente ejecutamos el Job utilizando el metodo run del objeto creado mediante la clase Processor. Debemos pasar las rutas de los buckets de Amazon S3 tanto para inputs (entradas) como para outputs(salidas). De esta forma SageMaker sabe de dónde tomar los datos de entrada y en dónde colocar los archivos resultantes de ejecutar el Job de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_parameters = {\n",
    "    'inputs':[ProcessingInput(input_name='input',\n",
    "                    source=f's3://{bucket}/{datasets_prefix}',\n",
    "                    destination='/opt/ml/processing/input'),\n",
    "              ProcessingInput(input_name='code',\n",
    "                    source=data_prep_script_path,\n",
    "                    destination='/opt/ml/processing/input/code')],\n",
    "    'outputs':[ProcessingOutput(output_name='train_data',\n",
    "                    source=f'/opt/ml/processing/output/train_data',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/train_data'),\n",
    "               ProcessingOutput(output_name='train_target',\n",
    "                    source=f'/opt/ml/processing/output/train_target',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/train_target'),\n",
    "               ProcessingOutput(output_name='test_data',\n",
    "                    source=f'/opt/ml/processing/output/test_data',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/test_data'),\n",
    "               ProcessingOutput(output_name='test_target',\n",
    "                    source=f'/opt/ml/processing/output/test_target',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/test_target'),\n",
    "               ProcessingOutput(output_name='encoder',\n",
    "                    source=f'/opt/ml/processing/output/encoder',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/encoder')],\n",
    "    'arguments':['--test-size', '0.1',\n",
    "                 '--data-file', 'churn.txt',\n",
    "                 '--train-data-file', train_data_file,\n",
    "                 '--train-target-file', train_target_file,\n",
    "                 '--test-data-file', test_data_file,\n",
    "                 '--test-target-file', test_target_file,\n",
    "                 '--encoder-file', encoder_file]}\n",
    "\n",
    "processor.run(**data_prep_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Para entrenar un modelo en Amazon SageMaker, se debe crear un job de entrenamiento con la siguiente información:\n",
    "\n",
    "La URL del bucket de Amazon S3 en dónde se encuentran los datos que serán utilizados para el entrenamiento\n",
    "Los recursos de cómputo que queremos que Amazon SageMaker utilice para el entrenamiento del modelo. Los recursos de cómputo son instancias para Machine Learning administradas por Amazon SageMaker\n",
    "La URL del bucket de Amazon S3 en el cual queremos que se guarde el resultado del proceso de entrenamiento\n",
    "La ruta de Amazon Elastic Container Registry en dónde el código de entrenamiento está almacenado. Para conocer más acerca de las rutas de los algoritmos incluidos en Amazon SageMaker consultar la documentación\n",
    "Existen las siguientes alternativas para entrenar un modelo utilizando Amazon SageMaker:\n",
    "\n",
    "Utilizar un algoritmo incluido en Amazon SageMaker - si alguno de estos cubre tus necesidades, basta con proporcionarle los datos e hiperpárametros que este require. Para obtener una lista de los algoritmos incluidos consultar la documentación\n",
    "\n",
    "Utilizar tu propio código de entrenamiento con algún framework soportado por Amazon SageMaker - los frameworks soportados son: TensorFlow, Pytorch, MXNet, Chainer, Scikit-learn y la biblioteca XGBoost\n",
    "\n",
    "Utilizar tu propio algoritmo - colocando tu código en una imagen Docker con todas las dependencias necesarias. Para mayor información consultar la documentación\n",
    "\n",
    "Utilizar algún algoritmo mediante una suscripción a través de AWS Marketplace - para mayor información acerca de como funcionan de estas suscripciones consultar la documentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de crear tres scripts distintos para cada algoritmo, crearemos uno sólo el cual a partir de los parámetros recibidos, entrenará el modelo con el algoritmo indicado. Utilizaremos la misma técnica de k-fold Cross-Validation utilizada en el Jupyter Notebook descargado de Introducción .\n",
    "\n",
    "Adicionalmente, este script nos servirá no solamente para el entrenamiento sino también para el despliegue posterior de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script_file = 'code/train_and_serve.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%writefile $training_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Carga el modelo en memoria\n",
    "def model_fn(model_dir):\n",
    "    print('Cargando modelo: model_fn')\n",
    "    clf = read_pkl(os.path.join(model_dir, \"model.pkl\"))\n",
    "    return clf\n",
    "\n",
    "# Deserealiza el body de la petición para poder generar las predicciones\n",
    "def input_fn(request_body, request_content_type):\n",
    "\n",
    "    if request_content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)\n",
    "        input_data = pd.DataFrame.from_dict(input_data)\n",
    "        # TODO: Es importante asegurarse de que las columnas se encuentran en el orden adecuado\n",
    "        return input_data\n",
    "        \n",
    "    elif request_content_type == 'text/csv':      \n",
    "        input_data = io.StringIO(request_body)        \n",
    "        return pd.read_csv(input_data, header=None)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "                \n",
    "# Genera la predicción sobre el objeto deserializado, con el modelo previamente cargado en memoria\n",
    "def predict_fn(input_data, model):\n",
    "    predict_proba = getattr(model, 'predict_proba', None)\n",
    "    if callable(predict_proba):\n",
    "        return predict_proba(input_data)[:, 1]\n",
    "    else:\n",
    "        return model.predict(input_data)\n",
    "\n",
    "# Serializa el resultado de la predicción al correspondiente content type deseado\n",
    "def output_fn(predictions, response_content_type):\n",
    "    if response_content_type == 'application/json':        \n",
    "        return json.dumps(predictions.tolist())\n",
    "    elif response_content_type == 'text/csv':\n",
    "        predictions_response = io.StringIO()\n",
    "        np.savetxt(predictions_response, predictions, delimiter=',')\n",
    "        return predictions_response.getvalue()\n",
    "    else:\n",
    "        raise ValueError(\"El endpoint del modelo solamente soporta Content-Types: 'application/json' o 'text/csv' como respuesta\")        \n",
    "        \n",
    "def read_pkl(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def to_pkl(data, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def random_forest(**hyperparameters):\n",
    "    return RandomForestClassifier(n_jobs=-1, \n",
    "                                  min_samples_split=hyperparameters['min_samples_split'],\n",
    "                                  n_estimators=hyperparameters['n_estimators'],\n",
    "                                  max_depth=hyperparameters['max_depth'],\n",
    "                                  max_features=hyperparameters['max_features'])\n",
    "\n",
    "def invalid_algorithm(**hyperparameters):\n",
    "    raise Exception('Invalid Algorithm')\n",
    "    \n",
    "def algorithm_selector(algorithm, **hyperparameters):\n",
    "    algorithms = {\n",
    "        'RandomForest': random_forest\n",
    "        }\n",
    "    \n",
    "    clf = algorithms.get(algorithm, invalid_algorithm)    \n",
    "    return clf(**hyperparameters)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    print(f'INFO: {script_name}: Iniciando entrenamiento del modelo')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train-data', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_DATA'))\n",
    "    parser.add_argument('--train-target', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_TARGET'))\n",
    "    \n",
    "    parser.add_argument('--algorithm', type=str)\n",
    "    parser.add_argument('--splits', type=int, default=10)\n",
    "    parser.add_argument('--target-metric', type=str)\n",
    "    \n",
    "    parser.add_argument('--learning-rate', type=float)\n",
    "    parser.add_argument('--min-samples-split', type=int)\n",
    "    parser.add_argument('--n-estimators', type=int)\n",
    "    parser.add_argument('--max-depth', type=int)\n",
    "    parser.add_argument('--max-features', type=int)\n",
    "    \n",
    "            \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(f'INFO: {script_name}: Parametros recibidos: {args}')\n",
    "\n",
    "    # Cargar datasets\n",
    "    files = os.listdir(args.train_data)\n",
    "    if len(files) == 1:\n",
    "        train_data = pd.read_csv(os.path.join(args.train_data, files[0]))\n",
    "    else:\n",
    "        raise Exception('Mas de un archivo recibido para el channel Data')\n",
    "    \n",
    "    files = os.listdir(args.train_target)\n",
    "    if len(files) == 1:\n",
    "        train_target = pd.read_csv(os.path.join(args.train_target, files[0]))\n",
    "        train_target = train_target['Churn'].tolist()\n",
    "    else:\n",
    "        raise Exception('Mas de un archivo recibido para el channel Target')\n",
    "\n",
    "     \n",
    "    clf = algorithm_selector(args.algorithm,\n",
    "                             min_samples_split=args.min_samples_split,\n",
    "                             n_estimators=args.n_estimators,\n",
    "                             max_depth=args.max_depth,\n",
    "                             max_features=args.max_features)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=args.splits)    \n",
    "    cv_scores = cross_validate(clf, train_data, train_target, cv=skf, scoring=args.target_metric, n_jobs=-1)\n",
    "    print('{} = {}%'.format(args.target_metric, cv_scores['test_score'].mean().round(4)*100))\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    clf.fit(train_data, train_target) \n",
    "    \n",
    "    # Guardar modelo\n",
    "    to_pkl(clf, os.path.join(args.model_dir, 'model.pkl'))\n",
    "\n",
    "    print(f'INFO: {script_name}: Finalizando el entrenamiento del modelo')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empaquetamos el script en un archvio .tar.gz y lo subimos a un bucket de Amazon S3 para poder utilizarlo en el job de entrenamiento y posteriormente para el despliegue del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script_tar_file = os.path.join('code',os.path.splitext(os.path.basename(training_script_file))[0] + '.tar.gz')\n",
    "\n",
    "sagemaker_utils.create_tar_gz(training_script_file, training_script_tar_file)\n",
    "\n",
    "training_script_path = sagemaker_utils.upload(training_script_tar_file, f's3://{bucket}/{code_prefix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear el Job de entrenamiento en Amazon SageMaker utilizamos la clase CustomEstimator (descargada con las dependencias) para crear un Estimator el cual permita integrar nuestro script train_and_serve.py con el contenedor Docker que previamente creamos para el entrenamiento de nuestros modelos.\n",
    "\n",
    "También utilizaremos la expresión regular recall = (\\d+\\.\\d{1,2})? para definir una métrica de desempeño de nuestro algoritmo, en este caso por tratarse de un problema de riesgo, lo que buscamos es incrementar el Recall, esta métrica es calculada mediante el uso de k-Fold Cross-Validation. Amazon SageMaker aplica esta expresión regular a los mensajes de la salida estándar. Posteriormente mediante el uso de esta métrica podremos crear un Job de optimización de hiperparámetros.\n",
    "\n",
    "Para ejecutar los procesos de entrenamiento lo haremos llamando el método fit del estimator previamente creado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve identified earlier that we are dealing with an imbalanced dataset and so we need to make sure we’re using the appropriate evaluation metrics for our case. For this reason, we’ll be looking at the common Accuracy metric with a grain of salt. To illustrate why this is the case, accuracy calculates the ratio of total truly predicted values to the total number of input samples, meaning that our model would get pretty high accuracy by predicting the majority class but would fail to capture the minority class, default, which is no bueno. This is why the evaluation metrics that we’ll be focusing on to assess the classification performance of our models are Precision, Recall and F1 score.\n",
    "\n",
    "Firstly, Precision gives us the ratio of true positives to the total positives predicted by a classifier where positives denote default cases in our context. Given that they’re the minority class in our dataset, we can see that our models do a good job at correctly predicting those minor instances. Moreover, Recall, a.k.a true positive rate, gives us the number of true positives divided by the total number of elements that actually belong to the positive class. In our case, Recall is a more important metric as opposed to Precision given that we’re more concerned with false negatives (our model predicting that someone is not gonna default but they do) than false positives (our model predicting that someone is gonna default but they don’t). Lastly, F1 Score provides a single score to measure both Precision and Recall. Now that we know what to look for, we can clearly see that XGboost performs the best across all 3 metrics. Although it scored better on Precision as opposed to Recall, it still has a pretty good F1 score of 0.81.\n",
    "\n",
    "We’ll now have a look at ROC which is a probability curve with False Positive Rate (FPR) on the x-axis and True Positive Rate (TPR, recall) on the y-axis. The best model should maximize the TPR to 1 and minimize FPR to 0. With this said, we can compare classifiers using the area under the curve of the ROC curve, AUC, where the higher its value, the better the model is at predicting 0s as 0s and 1s as 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'RandomForest':{}}\n",
    "metric_name = 'cross-val:recall'\n",
    "metric_regex = 'recall = (\\d+\\.\\d{1,2})?'\n",
    "\n",
    "for algorithm in estimators:   \n",
    "    estimators[algorithm] = Estimator(\n",
    "        image_uri = docker_images['Training']['image_uri'],        \n",
    "        entry_point = os.path.basename(training_script_file),\n",
    "        source_dir = training_script_path,\n",
    "        role = sagemaker_role,\n",
    "        instance_count = 1,\n",
    "        instance_type = 'ml.m5.xlarge',\n",
    "        output_path = f's3://{bucket}/{model_prefix}',\n",
    "        metric_definitions = [{'Name': metric_name, 'Regex': metric_regex}],\n",
    "        volume_size = 5,\n",
    "        max_run = 60*60*2, # dos horas\n",
    "        hyperparameters={\n",
    "            'algorithm':algorithm,\n",
    "            'splits':5,\n",
    "            'target-metric':'recall',\n",
    "            'learning-rate': 0.1, \n",
    "            'min-samples-split': 3, \n",
    "            'n-estimators': 300,\n",
    "            'max-depth': 25,\n",
    "            'max-features':20})\n",
    "    \n",
    "    estimators[algorithm].fit(\n",
    "        {'train_data': sagemaker_utils.get_processor_output_path(processor, 'train_data'),\n",
    "        'train_target': sagemaker_utils.get_processor_output_path(processor, 'train_target')},\n",
    "        wait=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esperar a que terminen de ejecutarse los procesos de entrenamiento en Amazon SageMaker, podemos utilizar el siguiente método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_utils.wait_for_training_jobs(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in estimators:\n",
    "    metrics = estimators[estimator].training_job_analytics.dataframe()\n",
    "    test_recall = metrics[metrics['metric_name'] == metric_name]['value'].values[0]\n",
    "    print(f'{estimator}: cross-val:recall = {test_recall}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker nos ofrece la funcionalidad de optimización de hiperpárametros, ya sea utilizando una búsqueda aleatoria o un método bayesiano, en este caso vamos a utilizar el segundo método el cual permite entrenar un modelo de forma iterativa e ir identificando que combinación de hiperparámetros, nos permite minimizar o maximizar más la métrica objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuners = {}\n",
    "\n",
    "total_jobs = 16\n",
    "parallel_jobs = 2\n",
    "\n",
    "tuners['RandomForest'] = HyperparameterTuner(\n",
    "        estimator=estimators['RandomForest'],\n",
    "        objective_metric_name=metric_name,\n",
    "        objective_type='Maximize',\n",
    "        hyperparameter_ranges={'min-samples-split': IntegerParameter(3,10), \n",
    "                               'n-estimators': IntegerParameter(150,300),\n",
    "                               'max-depth': IntegerParameter(20,35),\n",
    "                               'max-features': IntegerParameter(15,30)},\n",
    "        metric_definitions=[{'Name': metric_name, \n",
    "                              'Regex': metric_regex}],\n",
    "        max_jobs=total_jobs,\n",
    "        max_parallel_jobs=parallel_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable total_jobs especifíca el número total de procesos de entrenamiento (combinaciones distintas de valores de hiperparámetros) a ejecutar y la variable parallel_jobs especifíca el número máximo de procesos a ejecutar en paralelo.\n",
    "\n",
    "Para ejecutar cada uno de los procesos utilizamos el método fit() pasando como parámetros la ubicación de los datasets y mediante el parámetro wait=False es que le indicamos que ejecute el proceso de manera asíncrona (sin esperar a que cada uno de los procesos termine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tuner in tuners:\n",
    "    tuners[tuner].fit({'train_data': sagemaker_utils.get_processor_output_path(processor, 'train_data'),\n",
    "                       'train_target': sagemaker_utils.get_processor_output_path(processor, 'train_target')}, \n",
    "                      job_name= f'{prefix}-{tuner}-{strftime(\"%M-%S\", gmtime())}',\n",
    "                      wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_utils.wait_for_optmimization_jobs(tuners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la invocación del método describe() podemos obtener el proceso de entrenamiento cuyo modelo resultante obtuvo el mejor desempeño, en este caso maximizando la métrica objetivo Recall. Posteriormente de los metadatos devueltos del mejor proceso de entrenamiento, podemos obtener los valores de los hiperparámetros así cómo el valor obtenido en la métrica objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "for tuner in tuners:\n",
    "    best_training_job = tuners[tuner].describe()['BestTrainingJob']\n",
    "    objective_metric = best_training_job['FinalHyperParameterTuningJobObjectiveMetric']\n",
    "    \n",
    "    hyperparameters[tuner] = best_training_job['TunedHyperParameters']\n",
    "    print(tuner)\n",
    "    print(f\"\\thyper parameters: {hyperparameters[tuner]}\")\n",
    "    print(f\"\\t{objective_metric['MetricName']}: {objective_metric['Value']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el mejor candidato (modelo con mejor desempeño) para cada uno de los tres algoritmos, podemos pasar a comparar el desempeño entre estos para seleccionar el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluación de desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que cómo hicimos con la preparación de los datos, utilizaremos un Processing Job de Amazon SageMaker pero en este caso para buscar el umbral de clasificación, es decir el modelo nos devuelve la probabilidad de que el cliente haga Churn y lo que queremos en este caso es encontrar el valor a partir del cual clasificaremos como 1 (churn) o 0 (no-churn). Esto lo haremos buscando maximizar la métrica Recall pero manteniendo un mínimo valor para la métrica Precision, que recibiremos como parámetro.\n",
    "\n",
    "En el caso del Jupyter Notebook descargado podemos encontrar que esto se logra mediante la invocación de la siguiente función:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la que básicamente se obtienen las predicciones del modelos y posteriormente mediante el uso de la función precision_recall_curve() del Framework Scikit-learn y después a través del método argmax() de la librería numpy obtenemos el punto en el cual la métrica Recall se maximiza manteniendo el mínimo de la métrica Precision buscado.\n",
    "\n",
    "El programa que crearemos para el Processing Job implementará una lógica similar y al final guardaremos los umbrales encontrados para cada modelo en un archivo separado por comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models_script_file = 'code/evaluate_models.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $evaluate_models_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def load_model(file, model_file='model.pkl'):\n",
    "    if file.endswith('tar.gz'):\n",
    "        with tarfile.open(file, 'r:gz') as tar:\n",
    "            for name in tar.getnames():\n",
    "                if name == model_file:\n",
    "                    f = tar.extractfile(name)\n",
    "                    return pickle.load(f)\n",
    "            return None\n",
    "    elif file.endswith('pkl'):\n",
    "        with open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    print(f'INFO: {script_name}: Iniciando la evaluación de los modelos')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--algos', type=str, required=True)\n",
    "    parser.add_argument('--min-precision', type=float, required=True)    \n",
    "    parser.add_argument('--test-data-file', type=str, required=True)\n",
    "    parser.add_argument('--test-target-file', type=str, required=True)\n",
    "    parser.add_argument('--thresholds-file', type=str, required=True)   \n",
    "    parser.add_argument('--metrics-report-file', type=str, required=True)    \n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    print(f'INFO: {script_name}: Parámetros recibidos: {args}')\n",
    "    \n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    # Cargar datasets\n",
    "    test_target_path = os.path.join(input_path, 'target', args.test_target_file)     \n",
    "    test_target = pd.read_csv(test_target_path)\n",
    "    \n",
    "    test_data_path = os.path.join(input_path, 'data', args.test_data_file)     \n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Umbrales de decision por algoritmo\n",
    "    algo_metrics = {'Algorithm':[], 'Threshold':[], 'Precision':[], 'Recall':[]}\n",
    "    \n",
    "    metrics_report = {}\n",
    "    \n",
    "    algos = args.algos.split(',')\n",
    "    for algo in algos:\n",
    "        model_path = os.path.join(input_path, algo, 'model.tar.gz')         \n",
    "\n",
    "        # Carga modelo en memoria\n",
    "        print(f'Cargando modelo: {model_path}')\n",
    "        clf = load_model(model_path)\n",
    "        \n",
    "        # Obtiene predicciones con dataset para pruebas\n",
    "        predictions = clf.predict_proba(test_data)[:, 1]\n",
    "        \n",
    "        # Busca umbral de decision\n",
    "        precision, recall, thresholds = precision_recall_curve(test_target, predictions)\n",
    "        operating_point_idx = np.argmax(precision>=args.min_precision)\n",
    "        \n",
    "        algo_metrics['Threshold'].append(thresholds[operating_point_idx])\n",
    "        algo_metrics['Precision'].append(precision[operating_point_idx])\n",
    "        algo_metrics['Recall'].append(recall[operating_point_idx])\n",
    "        algo_metrics['Algorithm'].append(algo)\n",
    "        \n",
    "        metrics_report[algo] = {\n",
    "            'precision': {'value': precision[operating_point_idx], 'standard_deviation': 'NaN'},\n",
    "            'recall': {'value': recall[operating_point_idx], 'standard_deviation': 'NaN'}}\n",
    "         \n",
    "    \n",
    "    # Guardar Thresholds    \n",
    "    metrics = pd.DataFrame(algo_metrics)\n",
    "    print(f'INFO: {script_name}: Thresholds encontrados')\n",
    "    print(metrics)\n",
    "    metrics.to_csv(os.path.join(output_path, args.thresholds_file), index=False)    \n",
    "    \n",
    "    # Guardar reporte de metricas para cada modelo\n",
    "    for algo in metrics_report:\n",
    "        with open(os.path.join(output_path, f'{algo}_metrics.json'), 'w') as f:\n",
    "            json.dump({'binary_classification_metrics':metrics_report[algo]},f)        \n",
    "\n",
    "    \n",
    "    print(f'INFO: {script_name}: Finalizando la evaluación de los modelos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subimos el script a un bucket de Amazon S3 para poder utilizarlo con el Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models_script_path = sagemaker_utils.upload(evaluate_models_script_file, f's3://{bucket}/{code_prefix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que tenemos el script listo en S3, podemos pasar a crear el Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que lo hicimos cuando creamos el Processing Job para la preparación de los datos, en este caso creamos otro nuevamente utilizando la clase Processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(evaluate_models_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ejecutamos el proceso con el método run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_file = 'thresholds.csv'\n",
    "metrics_report_file = 'metrics_report.json'\n",
    "\n",
    "eval_parameters = {\n",
    "    'inputs':[ProcessingInput(\n",
    "                  input_name='code',\n",
    "                  source=evaluate_models_script_path,\n",
    "                  destination='/opt/ml/processing/input/code'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_processor_output_path(processor, 'test_target'), \n",
    "                  destination='/opt/ml/processing/input/target'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_processor_output_path(processor, 'test_data'), \n",
    "                  destination='/opt/ml/processing/input/data'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_tuner_best_model_artifacts_path(tuners['GradientBoosting']), \n",
    "                  destination='/opt/ml/processing/input/GradientBoosting'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_tuner_best_model_artifacts_path(tuners['RandomForest']),\n",
    "                  destination='/opt/ml/processing/input/RandomForest'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_tuner_best_model_artifacts_path(tuners['ExtraTrees']), \n",
    "                  destination='/opt/ml/processing/input/ExtraTrees')],\n",
    "    'outputs':[ProcessingOutput(\n",
    "                   output_name='eval',\n",
    "                   source='/opt/ml/processing/output',\n",
    "                   destination=f's3://{bucket}/{eval_prefix}')],\n",
    "    'arguments':['--algos', ','.join(estimators.keys()),\n",
    "                 '--min-precision', '0.85',\n",
    "                 '--test-data-file', test_data_file,\n",
    "                 '--test-target-file', test_target_file,\n",
    "                 '--thresholds-file', thresholds_file,\n",
    "                 '--metrics-report-file', metrics_report_file]}\n",
    "\n",
    "evaluation_processor.run(**eval_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar automaticamente el mejor modelo podemos descargar uno de los archivos resultantes del proceso de evaluación, en el cual se almacenaron los umbrales de decisión y seleccionar aquel que haya obtenido el valor más alto de Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_path = sagemaker_utils.get_processor_output_path(evaluation_processor,'eval')\n",
    "metrics = sagemaker_utils.read_csv(f'{thresholds_path}/{thresholds_file}')\n",
    "\n",
    "max_recall = metrics[metrics['Recall']==metrics['Recall'].max()]\n",
    "best_model_found = max_recall.loc[max_recall['Precision'].idxmax()]\n",
    "\n",
    "best_model_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despliegue del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegar el modelo exponiendolo como un endpoint el cual permita recibir peticiones HTTPs tipo REST mediante el uso de Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desplegar un modelo con Amazon SageMaker consiste en tres pasos:\n",
    "\n",
    "Crear un modelo en SageMaker – al crear un modelo, se le especifica a Amazon SageMaker en dónde encontrar los componentes del modelo. Esto incluye la ruta del bucket de Amazon S3 en dónde los artefactos del modelo se encuentran almacenados. Estos deben estar empaquetados en un archivo con nombre model.tar.gz. Así como la ruta de Amazon Elastic Container Registry en dónde se encuentra la imagen Docker que contiene el código para la inferencia\n",
    "Crear una configuración para el endpoint – se especifíca el nombre del modelo a utilizar y las instancias de cómputo para Machine Learning que se desea que Amazon SageMaker lance para hostear el modelo. Es posible configurar el endpoint para que automáticamente escale el número de instancias aprovisionadas. Cuando se especifican dos o más instancias, Amazon SageMaker las lanza en Zonas de Disponibilidad distintas y gestiona el reemplazo de las instancias cuando es necesario, esto asegura una disponibilidad continua\n",
    "Crear un endpoint HTTPS – a través de la configuración del endpoint proporcionada, Amazon SageMaker aprovisiona las instancias de cómputo requeridas y despliega el modelo conforme a la especificación de la configuración. Para obtener predicciones, la aplicación cliente puede realizar una petición al endpoint a través del protocolo HTTPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer el deployment del modelo seleccionado basta con invocar el método deploy() del objeto Estimator, del proceso de entrenamiento que mejor desempeño tuvo para ese algoritmo, proporcionando los siguientes parámetros:\n",
    "\n",
    "endpoint_name – nombre del endpoint a crear\n",
    "initial_instance_count – número inicial de instancias a utilizar en el clúster del endpoint\n",
    "instance_type – tipo de instancia(s) a utilizar\n",
    "image_uri – imagen Docker a utilizar. En este caso la creada para el despliegue del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'{prefix}-best-model-{strftime(\"%M-%S\", gmtime())}'\n",
    "\n",
    "best_estimator = tuners[best_model_found['Algorithm']].best_estimator()\n",
    "\n",
    "predictor = best_estimator.deploy(endpoint_name=endpoint_name,\n",
    "                                  initial_instance_count=1, \n",
    "                                  instance_type='ml.m5.large',\n",
    "                                  entry_point = os.path.basename(training_script_file),\n",
    "                                  source_dir = training_script_path,\n",
    "                                  image_uri=docker_images['Inference']['image_uri'])\n",
    "\n",
    "print(f'\\nEndpoint Name: {endpoint_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener predicciones mediante la invocación del endpoint creado a partir del despligue del modelo realizado, con la finalidad de evaluar el desempeño del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de realizar las peticiones al endpoint, descargaremos de Amazon S3 el dataset que utilizaremos para esta prueba. Para esto utilizaremos el método S3Downloader.download indicandole la ruta del archivo en el bucket de Amazon S3 así como la ruta local en dónde queremos que sea descargado el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sagemaker_utils.read_csv(f'{sagemaker_utils.get_processor_output_path(processor, \"test_data\")}/{test_data_file}')\n",
    "test_target = sagemaker_utils.read_csv(f'{sagemaker_utils.get_processor_output_path(processor, \"test_target\")}/{test_target_file}')\n",
    "\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos el dataset que utilizaremos para obtener predicciones del endpoint, podemos realizar peticiones a este utilizando el cliente de runtime de Amazon SageMaker mediante el uso del SDK para Python de AWS llamado boto3.\n",
    "\n",
    "El código a continuación es completamente independiente y podría ser ejecutado desde cualquier otro servicio de AWS, como por ejemplo AWS Lambda o Amazon EC2. O desde la PC local, instalando previamente el SDK boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "request_body = json.dumps(test_data.values.tolist())\n",
    "\n",
    "response = sagemaker_runtime.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                             ContentType = 'application/json',\n",
    "                                             Body = request_body)\n",
    "\n",
    "predictions = json.loads(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos obtenido las predicciones utilizando el dataset, podemos comparar estas contra las etiquetas reales y medir el desempeño"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparar predicción vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_set, predictions, classes, title):\n",
    "    classes=np.array(classes)\n",
    "    cm = confusion_matrix(test_set, predictions)\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, gridspec_kw={'width_ratios': [2, 3]})\n",
    "    im = ax1.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax1.figure.colorbar(im, ax=ax1)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax1.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=\"Confusion Matrix\",\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax1.get_yticklabels(), rotation=90, ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax1.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    precision = tp/(fp+tp)\n",
    "    recall = tp/(fn+tp)\n",
    "    specifity = tn/(tn+fp)\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.text(0,0.9, s='The overall model accuracy is {}% [ACCURACY]'.format(round(accuracy*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    ax2.text(0,0.7, s='Out of the customers the model predicted as will churn, {}% will actually churn [PRECISION]'.format(round(precision*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    ax2.text(0,0.5, s='The model will catch {}% of the customers who will actually churn [RECALL / SENSITIVITY]'.format(round(recall*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    ax2.text(0,0.3, s='The model will catch {}% of the customers who will actually NOT churn [SPECIFITY]'.format(round(specifity*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    fig.set_figheight(3)\n",
    "    fig.set_figwidth(16)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de poder graficar la matriz de confusión, debemos cambiar las predicciones obtenidas las cuales representan el porcentaje de probabilidad de hacer Churn, por un 0 o un 1 utilizando el umbral obtenido previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold = best_model_found['Threshold']\n",
    "predictions=[1 if prediction >= decision_threshold else 0 for prediction in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y finalmente podemos crear la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Not Churn','Churn']\n",
    "plot_confusion_matrix(test_target, predictions, labels, 'Best Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatización del Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrar todos los componentes previamente construidos en un workflow orquestado por Amazon SageMaker Pipelines y que permita ejecutar todo el pipeline sin necesidad de depender del Jupyter Notebook.\n",
    "\n",
    "Una vez que terminemos, el resultado será el pipeline que se muestra en la imagen, la representación gráfica es generada por Amazon SageMaker Pipelines. Lo crearemos reutilizando los componentes previamente creados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Model Building Pipelines permite construir pipelines de Machine Learning tomando ventaja de su integración directa con SageMaker, gracias a esto es posible crear un pipeline y configurar SageMaker Projects para orquestar el despliegue de los modelos.\n",
    "\n",
    "Utilizaremos Amazon SageMaker Model Building Pipelines para crear un workflow que nos permita integrar y automatizar todos los pasos antes creados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de parámetros del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = ParameterString(name='DatasetPath', default_value=f's3://{bucket}/{datasets_prefix}')\n",
    "model_approval_status = ParameterString(name='ModelApprovalStatus', default_value='PendingManualApproval')  # \"Approved\" Si no se requiere aprobación manual\n",
    "minimum_precision = ParameterFloat(name='MinimumPrecision', default_value=0.85)\n",
    "\n",
    "parameters_list = [dataset_path, model_approval_status, minimum_precision]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar paso al pipeline para ejecutar Processing Job para la preparación del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_step_parameters = {\n",
    "    'name':'Preparacion-de-Datos',\n",
    "    'processor':processor}\n",
    "\n",
    "data_prep_step_parameters.update(data_prep_parameters)\n",
    "data_prep_step_parameters['job_arguments'] = data_prep_step_parameters.pop('arguments')\n",
    "\n",
    "data_prep_step_parameters['inputs']=[ProcessingInput(input_name='input',\n",
    "                                         source=dataset_path,\n",
    "                                         destination='/opt/ml/processing/input'),\n",
    "                                     ProcessingInput(input_name='code',\n",
    "                                         source=data_prep_script_path,\n",
    "                                         destination='/opt/ml/processing/input/code')]\n",
    "\n",
    "data_prep_step = ProcessingStep(**data_prep_step_parameters)\n",
    "pipeline_steps = [data_prep_step]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar paso al pipeline para entrenamiento de los modelos utilizando Training Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = {}\n",
    "for algorithm in estimators:       \n",
    "    training_steps[algorithm] = TrainingStep(\n",
    "        name=f'Entrenamiento-con-{algorithm}',\n",
    "        estimator=tuners[algorithm].best_estimator(),\n",
    "        inputs={\n",
    "            'train_data': TrainingInput(\n",
    "                data_prep_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri),\n",
    "            'train_target': TrainingInput(\n",
    "                data_prep_step.properties.ProcessingOutputConfig.Outputs['train_target'].S3Output.S3Uri)})\n",
    "    \n",
    "    pipeline_steps.append(training_steps[algorithm])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar paso al pipeline para evaluación de desempeño de los modelos, utilizando un Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_files = {}\n",
    "\n",
    "for algorithm in estimators:\n",
    "    property_file = PropertyFile(\n",
    "        name=f'{algorithm}Metrics',\n",
    "        output_name=\"eval\",\n",
    "        path=f'{algorithm}_metrics.json')\n",
    "        \n",
    "    property_files[algorithm] = property_file\n",
    "\n",
    "eval_step_parameters = {\n",
    "    'name':'Evaluacion-de-modelos',\n",
    "    'processor':evaluation_processor,\n",
    "    'property_files':[property_files[file] for file in property_files]}\n",
    "\n",
    "eval_step_parameters.update(eval_parameters)\n",
    "eval_step_parameters['job_arguments'] = eval_step_parameters.pop('arguments')\n",
    "\n",
    "eval_step_parameters['inputs'] = [\n",
    "    ProcessingInput(\n",
    "        input_name='code',\n",
    "        source=evaluate_models_script_path,\n",
    "        destination='/opt/ml/processing/input/code'),\n",
    "    ProcessingInput(\n",
    "        source=data_prep_step.properties.ProcessingOutputConfig.Outputs['test_target'].S3Output.S3Uri, \n",
    "        destination='/opt/ml/processing/input/target'),\n",
    "    ProcessingInput(\n",
    "        source=data_prep_step.properties.ProcessingOutputConfig.Outputs['test_data'].S3Output.S3Uri, \n",
    "        destination='/opt/ml/processing/input/data'),\n",
    "    ProcessingInput(\n",
    "        source=training_steps['GradientBoosting'].properties.ModelArtifacts.S3ModelArtifacts, \n",
    "        destination='/opt/ml/processing/input/GradientBoosting'),\n",
    "    ProcessingInput(\n",
    "        source=training_steps['RandomForest'].properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        destination='/opt/ml/processing/input/RandomForest'),\n",
    "    ProcessingInput(\n",
    "        source=training_steps['ExtraTrees'].properties.ModelArtifacts.S3ModelArtifacts, \n",
    "        destination='/opt/ml/processing/input/ExtraTrees')]\n",
    "\n",
    "eval_step = ProcessingStep(**eval_step_parameters)\n",
    "pipeline_steps.append(eval_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar condición para registrar modelo en el Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_group_name = f'{prefix}-PackageGroup'\n",
    "\n",
    "for algorithm in estimators:   \n",
    "    model_metrics = ModelMetrics(\n",
    "        model_statistics = MetricsSource(\n",
    "            s3_uri=\"{}/{}_metrics.json\".format(\n",
    "                eval_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"],\n",
    "                algorithm),\n",
    "            content_type=\"application/json\"))\n",
    "    \n",
    "    register_step = RegisterModel(\n",
    "        name=f\"Registra{algorithm}\",\n",
    "        estimator=estimators[algorithm],\n",
    "        model_data=training_steps[algorithm].properties.ModelArtifacts.S3ModelArtifacts,\n",
    "        content_types=[\"text/csv\"],\n",
    "        response_types=[\"text/csv\"],\n",
    "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],\n",
    "        transform_instances=[\"ml.m5.large\"],\n",
    "        model_package_group_name=package_group_name,\n",
    "        approval_status=model_approval_status,\n",
    "        description=f'Churn prediction using {algorithm}',\n",
    "        model_metrics=model_metrics,\n",
    "        image_uri=docker_images['Inference']['image_uri'],\n",
    "        entry_point = training_script_file\n",
    "    )\n",
    "    \n",
    "    condition = ConditionGreaterThanOrEqualTo(\n",
    "        left = JsonGet(\n",
    "            step_name = 'Evaluacion-de-modelos',\n",
    "            property_file = property_files[algorithm],\n",
    "            json_path = f'binary_classification_metrics.precision.value'),\n",
    "        right = minimum_precision)\n",
    "    \n",
    "    condition_step = ConditionStep(\n",
    "        name=f\"{algorithm}Precision\",\n",
    "        conditions=[condition],\n",
    "        if_steps=[register_step],\n",
    "        else_steps=[])\n",
    "    \n",
    "    pipeline_steps.append(condition_step)\n",
    "    \n",
    "print(f'Package Group Name: {package_group_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar la celda veremos un resultado como el siguiente: Package Group Name: churn-clf-PackageGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(name=f'{prefix}-pipeline-{strftime(\"%M-%S\", gmtime())}',\n",
    "                    parameters=parameters_list,\n",
    "                    steps=pipeline_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genera definición del pipeline para ver que no exista ningún problema, si no arroja ningún error la ejecución de la siguiente celda, todo está bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea o actualiza un pipeline en SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicia la ejecución del pipeline creado. Al no especificar valores de parámetros al invocar el método start, el pipeline es ejecutado con los valores por default definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista los pasos que se han ejecutado del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Despliegue del modelo(ver 10.4)\n",
    "\n",
    "\n",
    "Después de haberse ejecutado el pipeline, tendremos 3 versiones del modelo listas para nuestra revisión en el Model Registry y queremos que una vez que alguna de estas versiones sea aprobada, se despliegue como un Endpoint de SageMaker. Esto lo lograremos mediante el uso de SageMaker Projects.\n",
    "\n",
    "Un SageMaker Project es en realidad un producto aprovisionado mediante un template de AWS Service Catalog con el cual se pueden construir soluciones de punta a punta con Machine Learning.\n",
    "\n",
    "Es posible utilizar Amazon SageMaker Projects para generar modelos basados en triggers, como por ejemplo cuando alguien realiza un check in de un cambio de código.\n",
    "\n",
    "En este caso utilizaremos uno de los Project templates que SageMaker proporciona, llamado MLOps template for model deployment.\n",
    "\n",
    "Debemos seleccionar el último ícono que aparece en el menú de la izquierda “SageMaker resources”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calendarización del Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calendarizar la ejecución automática del workflow previamente creado mediante la integración con Amazon EventBridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear rol de ejecución\n",
    "Creación de rol de ejecución a ser utilizado con la regla de EventBridge que en seguida crearemos para poder calendarizar la ejecución del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_document={\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [               \n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"sagemaker:StartPipelineExecution\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "pipeline_execution_role = sagemaker_utils.create_pipeline_execution_role('test_pipeline', policy_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado el rol que utilizaremos, podemos pasar a crear la regla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear la regla de Amazon EventBridge utilizamos el SDK de AWS para Python boto3 y mediante la invocación del método put_rule( ) creamos la regla, debemos proporcionar los siguientes parámetros:\n",
    "\n",
    "Name – nombre de la regla a crear\n",
    "ScheduleExpression – expresión que indica la frecuencia de ejecución de la regla. Para conocer mas detalles sobre la definición de estas expresiones, consultar la documentación\n",
    "State– estado de la regla, en este caso ENABLED para indicar que está activa\n",
    "Description – descripción de la regla a crear\n",
    "RoleArn – rol de ejecución a ser utilizado por la regla\n",
    "En este caso la regla será ejecutada 10 minutos después de su creación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = boto3.client('events')\n",
    "\n",
    "schedule_expression = 'cron({date:%M %H} * * ? *)'.format( date=datetime.datetime.now() + datetime.timedelta(minutes = 10) )\n",
    "\n",
    "rule_name = f'{prefix}-pipeline-execution'\n",
    "put_rule_response = events.put_rule(\n",
    "                        Name=rule_name,\n",
    "                        ScheduleExpression=schedule_expression,\n",
    "                        State='ENABLED',\n",
    "                        Description='Rule for scheduling pipeline execution',\n",
    "                        RoleArn=pipeline_execution_role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el momento sólo creamos la regla de Amazon EventBridge pero no hemos indicado que es lo que debe ejecutar la regla, que en este caso será el pipeline de SageMaker que creamos previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El último paso consiste en agregar el target a la regla, es decir lo que la regla ejecutará de acuerdo a su calendarización. En este caso lo que nos interesa ejecutar el es pipline previamente creado.\n",
    "\n",
    "Para agregarlo utilizamos el método put_targets como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.put_targets(\n",
    "    Rule=rule_name,\n",
    "    Targets=[{\n",
    "        'Id': pipeline.name,\n",
    "        'Arn': pipeline.describe()['PipelineArn'],\n",
    "        'RoleArn': pipeline_execution_role,\n",
    "        'SageMakerPipelineParameters': {\n",
    "            'PipelineParameterList': [\n",
    "                {\n",
    "                    'Name': 'DatasetPath',\n",
    "                    'Value': f's3://{bucket}/{datasets_prefix}'\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'ModelApprovalStatus',\n",
    "                    'Value': 'PendingManualApproval'\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'MinimumPrecision',\n",
    "                    'Value': '0.85'\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    }]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación y publicación de API para invocación de endpoint\n",
    "\n",
    "Objetivo\n",
    "\n",
    "Crear y publicar un API REST que permita simplificar la invocación del endpoint del modelo expuesto, permitiendo realizar las transformaciones necesarias a los datos previo a la invocación del endpoint para obtener las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de script con lógica para la función Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de layer para Sklearn y encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf lambda_function; mkdir lambda_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_function/app.py\n",
    "import boto3\n",
    "import sklearn\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections.abc import Mapping\n",
    "        \n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "encoder = None\n",
    "endpoint_name = None\n",
    "decision_threshold = None\n",
    "\n",
    "def read_pkl(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def download_s3_file(s3_path):\n",
    "    cmpnts = s3_path.split('/')\n",
    "    bucket = cmpnts[2]\n",
    "    key = '/'.join(cmpnts[3:])\n",
    "    local_file = '/tmp/{}'.format(cmpnts[-1])\n",
    "\n",
    "    s3.download_file(bucket, key, local_file)\n",
    "\n",
    "    return local_file\n",
    "    \n",
    "def init():\n",
    "    global encoder\n",
    "    global endpoint_name\n",
    "    global decision_threshold\n",
    "    \n",
    "    if encoder == None or endpoint_name == None or decision_threshold == None:             \n",
    "        endpoint_name = os.environ['ENDPOINT_NAME']\n",
    "        encoder_s3_path = os.environ['ENCODER_S3_PATH']\n",
    "        thresholds_s3_path = os.environ['THRESHOLDS_S3_PATH']\n",
    "        \n",
    "        print(f'endpoint_name={endpoint_name}')\n",
    "        print(f'encoder_s3_path={encoder_s3_path}')\n",
    "        print(f'thresholds_s3_path={thresholds_s3_path}')\n",
    "\n",
    "        encoder = read_pkl(download_s3_file(encoder_s3_path))\n",
    "        thresholds = pd.read_csv(download_s3_file(thresholds_s3_path))\n",
    "        \n",
    "        decision_threshold = thresholds.max()['Threshold']\n",
    "    \n",
    "\n",
    "def handler(event, context):\n",
    "    init()\n",
    "    response = {\n",
    "        'statusCode': 500} #Internal server error\n",
    "\n",
    "    \n",
    "    print(event)\n",
    "    \n",
    "    try:\n",
    "        body = json.loads(event['body'])\n",
    "        \n",
    "        # Crear DataFrame con los datos recibidos\n",
    "        if isinstance(body, Mapping):\n",
    "            data = pd.DataFrame(body.items()).transpose()\n",
    "            header = data.iloc[0]\n",
    "            data = pd.DataFrame(data[1:].values, columns=header)\n",
    "        elif isinstance(body, list):\n",
    "            data = pd.DataFrame(body)        \n",
    "        else:\n",
    "            print('ERROR: Input type not supporte: it should be a dictionary or a list')\n",
    "            return response                \n",
    "        \n",
    "        # Realizar one hot encoding de variables categóricas\n",
    "        columns = ['State','Area_Code']        \n",
    "        transformed = encoder.transform(data[columns]).toarray()\n",
    "        \n",
    "        data.drop(columns,axis=1, inplace=True)\n",
    "        data = pd.concat([data,pd.DataFrame(transformed, columns=encoder.get_feature_names())],axis=1)        \n",
    "        \n",
    "        # Reemplazar yes/no por 1/0 en columnas Int_l_Plan y VMail_Plan\n",
    "        data['Int_l_Plan'] = data['Int_l_Plan'].map(dict(yes=1, no=0))\n",
    "        data['VMail_Plan'] = data['VMail_Plan'].map(dict(yes=1, no=0))        \n",
    "\n",
    "        # Ordenar columnas\n",
    "        columns = 'Account_Length,Int_l_Plan,VMail_Plan,VMail_Message,Day_Mins,Day_Calls,'+\\\n",
    "              'Eve_Mins,Eve_Calls,Night_Mins,Night_Calls,Intl_Mins,Intl_Calls,CustServ_Calls,'+\\\n",
    "              'x0_AK,x0_AL,x0_AR,x0_AZ,x0_CA,x0_CO,x0_CT,x0_DC,x0_DE,x0_FL,x0_GA,x0_HI,x0_IA,'+\\\n",
    "              'x0_ID,x0_IL,x0_IN,x0_KS,x0_KY,x0_LA,x0_MA,x0_MD,x0_ME,x0_MI,x0_MN,x0_MO,x0_MS,'+\\\n",
    "              'x0_MT,x0_NC,x0_ND,x0_NE,x0_NH,x0_NJ,x0_NM,x0_NV,x0_NY,x0_OH,x0_OK,x0_OR,x0_PA,'+\\\n",
    "              'x0_RI,x0_SC,x0_SD,x0_TN,x0_TX,x0_UT,x0_VA,x0_VT,x0_WA,x0_WI,x0_WV,x0_WY,x1_408,x1_415,x1_510'\n",
    "\n",
    "        columns = columns.split(',')\n",
    "        data = data[columns]\n",
    "        \n",
    "        # Obtener predicciones\n",
    "        request_body = json.dumps(data.values.tolist())\n",
    "\n",
    "        response = sagemaker_runtime.invoke_endpoint(EndpointName = endpoint_name,\n",
    "                                                     ContentType = 'application/json',\n",
    "                                                     Body = request_body)\n",
    "\n",
    "        predictions = json.loads(response['Body'].read())\n",
    "        \n",
    "        predictions=[1 if prediction >= decision_threshold else 0 for prediction in predictions]\n",
    "        \n",
    "        response = {\n",
    "            'statusCode': 200,\n",
    "            'body':json.dumps({'predictions': predictions})}\n",
    "\n",
    "    except Exception as e:\n",
    "        print('ERROR: Executing endpoint: {}'.format(endpoint_name))\n",
    "        print(e)\n",
    "            \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de contenedor para función Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_images['Lambda']={'libraries':{'awslambdaric':'1.0.0',\n",
    "                                      'boto3':'1.17.77'},\n",
    "                         'dependencies':[('lambda_function','/function')],\n",
    "                         'others':['WORKDIR /function'],\n",
    "                         'entrypoint':['python','-m','awslambdaric'],\n",
    "                         'cmd':['app.handler']}\n",
    "\n",
    "lambda_docker_parameters = {'image_name': f'{prefix}-lambda',\n",
    "                            'base_image': docker_images['Processing']['image_uri'],\n",
    "                            's3_path': f's3://{bucket}/{images_directory}',\n",
    "                            'role': codebuild_role,  \n",
    "                            'secret': secret_name}\n",
    "\n",
    "lambda_docker_parameters.update(docker_images['Lambda'])\n",
    "docker_images['Lambda']['image_uri'] = sagemaker_utils.create_docker_image(**lambda_docker_parameters)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Creación de función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"sagemaker:*\",\n",
    "                    \"s3:*\",\n",
    "                    \"states:StartExecution\",\n",
    "                    \"iam:PassRole\",\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "lambda_role = sagemaker_utils.create_lambda_execution_role(f'{prefix}-LambdaExecutionRole', policy_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_function_name = f'{prefix}-endpoint-function'\n",
    "lambda_function_params = {\n",
    "    'FunctionName': endpoint_function_name,\n",
    "    'Role': lambda_role,\n",
    "    'Code': {'ImageUri':docker_images['Lambda']['image_uri']},\n",
    "    'Description': 'Function for invoking SageMaker Endpoints',\n",
    "    'Timeout': 300, # 5 mins\n",
    "    'MemorySize': 256, # MB\n",
    "    'PackageType': 'Image',\n",
    "    'Environment': {\n",
    "        'Variables': {\n",
    "            'ENDPOINT_NAME': 'Colocar aqui el nombre del endpoint',\n",
    "            'ENCODER_S3_PATH': f\"{sagemaker_utils.get_processor_output_path(processor,'encoder')}/{encoder_file}\",\n",
    "            'THRESHOLDS_S3_PATH': f\"{sagemaker_utils.get_processor_output_path(evaluation_processor,'eval')}/{thresholds_file}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "lambda_response = sagemaker_utils.create_lambda_function(**lambda_function_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de API\n",
    "\n",
    "Creación de API para exponer la función Lambda creada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apigateway = boto3.client('apigateway')\n",
    "\n",
    "api_creation_response = apigateway.create_rest_api(name='{}-api-{}'.format(prefix,strftime(\"%M-%S\", gmtime())),\n",
    "                           description='API for exposing Lambda function to invoke SageMaker Endpoint',\n",
    "                           endpointConfiguration={'types':['REGIONAL']})\n",
    "api_creation_response['id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear método POST para el API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_resource = apigateway.get_resources(restApiId=api_creation_response['id'])['items'][-1]\n",
    "\n",
    "apigateway.put_method(restApiId=api_creation_response['id'],\n",
    "                      resourceId=root_resource['id'],\n",
    "                      httpMethod='POST',\n",
    "                      authorizationType='NONE'\n",
    "                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_uri = 'arn:aws:apigateway:{}:lambda:path/2015-03-31/functions/{}/invocations'.format(session.boto_region_name,\n",
    "                                                                        lambda_response['FunctionArn'])\n",
    "apigateway.put_integration(restApiId=api_creation_response['id'],\n",
    "                           resourceId=root_resource['id'],\n",
    "                           httpMethod='POST',\n",
    "                           integrationHttpMethod='POST',\n",
    "                           type='AWS_PROXY',\n",
    "                           uri=lambda_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dar permisos a la función Lambda para poder ser invocada desde API Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client = boto3.client('lambda')\n",
    "lambda_client.add_permission(FunctionName=lambda_response['FunctionName'],\n",
    "                             StatementId='apigateway-post',\n",
    "                             Action='lambda:InvokeFunction',\n",
    "                             Principal='apigateway.amazonaws.com'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despliegue de API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_name = 'v1'\n",
    "apigateway.create_deployment(restApiId=api_creation_response['id'],\n",
    "                             stageName=stage_name,\n",
    "                             stageDescription='API v1 for Invoking SageMaker Endpoint'\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_url = 'https://{}.execute-api.{}.amazonaws.com/{}'.format(api_creation_response['id'], \n",
    "                                                                 session.boto_region_name,\n",
    "                                                                 stage_name)\n",
    "\n",
    "print('invoke URL: {}'.format(invoke_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probar el API REST para obtener predicciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "payload={\n",
    "  \"State\": \"LA\",\n",
    "  \"Account_Length\": 52,\n",
    "  \"Area_Code\": 408,\n",
    "  \"Int_l_Plan\": \"yes\",\n",
    "  \"VMail_Plan\": \"no\",\n",
    "  \"VMail_Message\": 31,\n",
    "  \"Day_Mins\": 223,\n",
    "  \"Day_Calls\": 98,\n",
    "  \"Eve_Mins\": 264,\n",
    "  \"Eve_Calls\": 136,\n",
    "  \"Night_Mins\": 158,\n",
    "  \"Night_Calls\": 73,\n",
    "  \"Intl_Mins\": 14,\n",
    "  \"Intl_Calls\": 6,\n",
    "  \"CustServ_Calls\": 3\n",
    "}\n",
    "\n",
    "data = json.dumps(payload).encode('utf8')\n",
    "req =  urllib.request.Request(invoke_url, data=data, headers={'content-type': 'application/json'})\n",
    "resp = urllib.request.urlopen(req)\n",
    "predictions = json.loads(resp.read())['predictions']\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo se obtienen las predicciones en modo batch con Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depués de entrenar un modelo, se puede hacer el despligue del mismo con Amazon SageMaker para obtener predicciones de dos formas:\n",
    "\n",
    "Creando un endpoint persistente el cual nos permite obtener predicciones en tiempo-real, utilizando los servicios de hosting de Amazon SageMaker\n",
    "Obteniendo predicciones en modo batch para un dataset completo, utilizando Amazon SageMaker batch transform\n",
    "Para poder obtener predicciones utilizando un proceso batch transform de Amazon SageMaker, el dataset para el cual se desea generar predicciones debe ser previamente almacenado en Amazon S3 y el resultado de las predicciones generadas también será almacenado en un bucket de Amazon S3. Amazon SageMaker batch transform gestiona todos los recursos de cómputo requeridos para obtener las predicciones, esto incluye el lanzar las instancias de cómputo y su eliminación una vez que el proceso batch transform haya terminado. Batch transform administra las interacciones entre el dataset y el modelo a través de un objeto dentro del nodo de la instancia llamado agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un proceso batch transform se debe proporcionar lo siguiente:\n",
    "\n",
    "La ruta al bucket de Amazon S3 en dónde están almacenados los datos que se desea transformar\n",
    "Los recursos de cómputo que se desea que Amazon SageMaker utilice para ejecutar el proceso batch de transformación. Los recursos son instancias de cómputo para Machine Learning que son administradas por Amazon SageMaker\n",
    "La ruta del bucket de Amazon S3 en el cúal se desea almacenar el resultado generado por el proceso\n",
    "El nombre del modelo en Amazon SageMaker que se desea utilizar para crear las inferencias. Se debe utilizar un modelo que previamente haya sido creado en Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un proceso de transformación (para obtener predicciones) en modo batch, primero debemos crear o registrar el modelo en SageMaker, esto lo hacemos mediante la clase SKLearnModel proporcionando los siguientes parámetros:\n",
    "\n",
    "image – ruta de Amazon Elastic Container Registry de la imagen Docker a utilizar\n",
    "model_data – ruta de Amazon S3 en dónde se encuentra el archivo model.tar.gz a desplegar\n",
    "role – rol de Identity and Access Management a utilizar\n",
    "entry_point – script Python a utilizar con la lógica para cargar el modelo en memoria y generar predicciones\n",
    "framework_version – versión del Scikit-learn a utilizar\n",
    "name – nombre del modelo\n",
    "Una vez creado el modelo, es posible crear el proceso batch mediante la invocación del método transform() del objeto modelo previamente creado. Debemos proporcionar los siguientes parámetros:\n",
    "\n",
    "instance_count – número de instancias de cómputo a utilizar\n",
    "instance_type – tipo de instancia de cómputo a tulizar\n",
    "strategy – la forma como deben enviarse los registros del dataset del cual se desean obtener las predicciones\n",
    "assemble_with – el delimitador para separar los registros del archivo de predicciones resultante\n",
    "accept – el content-type esperado como respuesta\n",
    "output_path – ruta de algún bucket de Amazon S3 en dónde queramos que se guarden las predicciones generadas\n",
    "Y finalmente podemos ejecutar el proceso mediante la invocación del método transform() del objeto transformer previamente creado. Debemos proporcionar los siguientes parámetros:\n",
    "\n",
    "data – ruta en Amazon S3 del archivo a utilizar para generar las predicciones\n",
    "split_type – delimitador de cada registro\n",
    "content_type – content type correspondiente al archivo a utilizar\n",
    "wait – indica si queremos esperar a que termine la ejecución del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "models = {}\n",
    "transformers = {}\n",
    "id = '{date:%f}'.format( date=datetime.datetime.now() )[-4:]\n",
    "\n",
    "for tuner in tuners:\n",
    "    training_job_description = sess.describe_training_job(job_name=tuners[tuner].best_training_job())\n",
    "    model_artifacts = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "    training_image = training_job_description['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "    models[tuner] = SKLearnModel(image_uri=training_image,\n",
    "                                 model_data=model_artifacts,\n",
    "                                 role=sagemaker_role,\n",
    "                                 entry_point='train_and_deploy.py',\n",
    "                                 framework_version='0.23-1',\n",
    "                                 name=f'{prefix}-{tuner}-{id}')\n",
    "\n",
    "    transformers[tuner] = models[tuner].transformer(instance_count=1,\n",
    "                                                    instance_type='ml.m5.large',\n",
    "                                                    strategy='MultiRecord',\n",
    "                                                    assemble_with='Line',\n",
    "                                                    accept='text/csv',\n",
    "                                                    output_path=f's3://{bucket}/{transformed_data_prefix}/{tuner}')\n",
    "\n",
    "    transformers[tuner].transform(data=f's3://{bucket}/{processed_data_prefix}/{test_data_file}', \n",
    "                                  split_type='Line', content_type='text/csv', wait=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con el siguiente método podemos esperar a que termine la ejecución de los tres procesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.wait_for_transform_jobs(transformers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatización del Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrar todos los componentes previamente construidos en un workflow orquestado por AWS Step Functions y que permita ejecutar todo el pipeline sin necesidad de depender del Jupyter Notebook.\n",
    "\n",
    "Una vez que terminemos, el resultado será el pipeline que se muestra en la animación, la representación gráfica es generada por AWS Step Functions. Lo crearemos reutilizando los componentes previamente creados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Step Functions permite coordinar múltiples servicios AWS en un workflow serverless. Utilizando AWS Step Functions, es posible ejecutar workflows que combinen servicios como Amazon SageMaker y AWS Lambda, entre otros. Los workflows constan de pasos, en los cuales la salida de un paso se convierte en la entrada del siguiente.\n",
    "\n",
    "El SDK de Step Functions para Data Science permite crear y ejecutar workflows de Machine Learning con AWS Step Functions directamente desde Python, y desde un Jupyter Notebook. Cuando la definición del workflow esta terminada, es subido al servicio AWS Step Functions para su ejecución en el Cloud. De esa forma, una vez creado o actualizado el workflow, este vive en el Cloud y puede ser reutilizado.\n",
    "\n",
    "El workflow puede ser ejecutado tantas veces como sea necesario, y opcionalmente es posible cambiar la entrada de datos del workflow en cada ejecución. Cada que el workflow es ejecutado, este crea una nueva instancia de ejecución en el Cloud. Es posible ejecutarlo varias veces en paralelo.\n",
    "\n",
    "A través del SDK es posible crear los pasos, encadenarlos juntos para un workflow, crear el workflow en AWS StepFunctions, y ejecutar el workflow en AWS cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que tenemos que hacer antes de crear el workflow para automatizar el pipeline, es crear dos roles que serán utilizados por los servicios AWS Step Functions y AWS Lambda.\n",
    "\n",
    "Empezamos por importar las dependencias que utilizaremos para la definición y ejecución del workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import stepfunctions\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_role_name = '{}-StepFunctionsWorkflowExecutionRole'.format(prefix) \n",
    "lambda_role_name = '{}-LambdaExecutionRole'.format(prefix)\n",
    "model_function_name = '{}-ModelFunction'.format(prefix)\n",
    "select_model_function_name = '{}-SelectModelFunction'.format(prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos la política de permisos para el Rol de Ejecución para AWS Step Functions, es decir para el workflow como tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_functions_policy_document={\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [           \n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"states:*\",\n",
    "                \"sagemaker:*\",\n",
    "                \"lambda:*\",\n",
    "                \"cloudwatch:*\",\n",
    "                \"events:*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_functions_asume_role_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\n",
    "            \"Service\": \"states.amazonaws.com\"\n",
    "        },\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "    }]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y utilizando el siguiente método para crear el Rol de Ejecución para AWS Step Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_role = utils.create_or_update_iam_role(role_name = workflow_role_name, \n",
    "                          role_desc = 'Execution role for Step Functions workflow', \n",
    "                          asume_role_policy_document = step_functions_asume_role_document,\n",
    "                          policy_name = 'StepFunctionsWorkflowExecutionPolicy',\n",
    "                          policy_document = step_functions_policy_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo el mismo procedimiento creamos el Rol de Ejecución para AWS Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:*\",\n",
    "                \"s3:*\",\n",
    "                \"states:StartExecution\",\n",
    "                \"iam:PassRole\",\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_asume_role_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\n",
    "            \"Service\": \"lambda.amazonaws.com\"\n",
    "        },\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "    }]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_role = utils.create_or_update_iam_role(role_name = lambda_role_name, \n",
    "                                        role_desc = 'Execution role for Lambda functions', \n",
    "                                        asume_role_policy_document = lambda_asume_role_document,\n",
    "                                        policy_name = 'LambdaExecutionPolicy',\n",
    "                                        policy_document = lambda_policy_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros del workflow\n",
    "\n",
    "Empezamos por definir los parámetros de entrada que recibirá el workflow, estos parámetros podremos posteriormente utilizarlos en la ejecución de los pasos del workflow y de esta forma parametrizar algunas cosas y volver más flexible nuestro pipeline. Esto lo hacemos mediante el uso de la función ExecutionInput() ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "    'ProcessingJobName': str,           \n",
    "    'GradientBoostingTuningJobName': str,\n",
    "    'RandomForestTuningJobName': str,\n",
    "    'ExtraTreesTuningJobName': str,\n",
    "    'GradientBoostingTransformJobName': str,\n",
    "    'RandomForestTransformJobName': str,\n",
    "    'ExtraTreesTransformJobName': str,\n",
    "    'GradientBoostingModelName': str,\n",
    "    'RandomForestModelName': str,\n",
    "    'ExtraTreesModelName': str,\n",
    "    'GradientBoostingEvaluationJobName': str,\n",
    "    'RandomForestEvaluationJobName': str,\n",
    "    'ExtraTreesEvaluationJobName': str,\n",
    "    'ModelLambdaFunctionName': str, \n",
    "    'EndpointConfigName': str,\n",
    "    'EndpointName': str\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subir scripts a S3\n",
    "Para poder agregar la creación de los Jobs de Amazon SageMaker para el procesamiento, entrenamiento, evaluación y despliegue; necesitamos subir a Amazon S3 los scripts necesarios para cada uno de estos procesos. Para esto utilizamos el método S3Uploader.upload( ) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "processing_code_path = S3Uploader.upload(local_path='processing.py',\n",
    "                                         desired_s3_uri='s3://{}/{}'.format(bucket, code_prefix),\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "with tarfile.open('train_and_deploy.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('train_and_deploy.py')\n",
    "    \n",
    "train_and_deploy_code_path = S3Uploader.upload(local_path='train_and_deploy.tar.gz',\n",
    "                                         desired_s3_uri='s3://{}/{}'.format(bucket, code_prefix),\n",
    "                                         sagemaker_session=sess)\n",
    "\n",
    "model_evaluation_code_path = S3Uploader.upload(local_path='evaluate_models.py',\n",
    "                                         desired_s3_uri='s3://{}/{}'.format(bucket, code_prefix),\n",
    "                                         sagemaker_session=sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez teniendo los scripts arriba, podemos empezar a crear cada uno de los pasos que agregaremos al workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un primer paso en el workflow, para el pre-procesamiento de los datos, utilizamos el método steps.sagemaker.ProcessingStep( )\n",
    "\n",
    "Proporcionando los siguientes parámetros:\n",
    "\n",
    "state_id – Identificador del estado (paso) del workflow, este aparecerá como el nombre en el diagrama del workflow\n",
    "processor – El processor que será utilizado para ejecutar el job, en este caso el que fue creado en 3.4 Crear Processing Job\n",
    "job_name – Nombre del job a crear, en este caso será el valor recibido en le parámetro ProcessingJobName del workflow\n",
    "inputs – Rutas de Amazon S3 de las cuales se tomarán los archivos de entrada para el proceso\n",
    "outputs – Ruta de Amazon S3 en la cual queremos que se deposite el resultado de la ejecución del proceso\n",
    "container_arguments – Parámetros que recibirá el container en el cual se ejecutará el script\n",
    "container_entrypoint – Ruta del script a ejecutar dentro del container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = steps.sagemaker.ProcessingStep(\n",
    "    state_id='Preparación de Datos',\n",
    "    processor=sklearn_processor,\n",
    "    job_name=execution_input['ProcessingJobName'],\n",
    "    inputs=[ProcessingInput(input_name='input',\n",
    "                            source='s3://{}/{}'.format(bucket, datasets_prefix), \n",
    "                            destination='/opt/ml/processing/input'),\n",
    "            ProcessingInput(input_name='code',\n",
    "                            source=processing_code_path, \n",
    "                            destination='/opt/ml/processing/input/code')],\n",
    "    outputs=[ProcessingOutput(output_name='output',\n",
    "                              source='/opt/ml/processing/output/data',\n",
    "                              destination='s3://{}/{}'.format(bucket, processed_data_prefix))],\n",
    "    container_arguments=['--test-size', '0.1',\n",
    "                         '--data-file', 'churn.txt',\n",
    "                         '--train-data-file', train_data_file,\n",
    "                         '--train-target-file', train_target_file,\n",
    "                         '--test-data-file', test_data_file,\n",
    "                         '--test-target-file', test_target_file,\n",
    "                         '--encoder-file', encoder_file],\n",
    "    container_entrypoint=['python3','/opt/ml/processing/input/code/processing.py'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f087cbbaaafa03b885b484966f26ebaef7143783b0ae783847677076d63b434"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('drift_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
