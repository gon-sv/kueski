{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo:\n",
    "\n",
    "entrenaremos tres distintos modelos utilizando los algoritmos del Framework Scikit-learn: Gradient Boosting, Extra Trees y Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de crear tres scripts distintos para cada algoritmo, crearemos uno sólo el cual a partir de los parámetros recibidos, entrenará el modelo con el algoritmo indicado. Utilizaremos la misma técnica de k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script_file = 'code/train_and_serve.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%writefile $training_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Carga el modelo en memoria\n",
    "def model_fn(model_dir):\n",
    "    print('Cargando modelo: model_fn')\n",
    "    clf = read_pkl(os.path.join(model_dir, \"model.pkl\"))\n",
    "    return clf\n",
    "\n",
    "# Deserealiza el body de la petición para poder generar las predicciones\n",
    "def input_fn(request_body, request_content_type):\n",
    "\n",
    "    if request_content_type == 'application/json':\n",
    "        input_data = json.loads(request_body)\n",
    "        input_data = pd.DataFrame.from_dict(input_data)\n",
    "\n",
    "        return input_data\n",
    "        \n",
    "    elif request_content_type == 'text/csv':      \n",
    "        input_data = io.StringIO(request_body)        \n",
    "        return pd.read_csv(input_data, header=None)\n",
    "    else:\n",
    "        raise ValueError(\"El endpoint del modelo solamente soporta Content-Types: 'application/json' o 'text/csv' como entrada\")\n",
    "                \n",
    "# Genera la predicción sobre el objeto deserializado, con el modelo previamente cargado en memoria\n",
    "def predict_fn(input_data, model):\n",
    "    predict_proba = getattr(model, 'predict_proba', None)\n",
    "    if callable(predict_proba):\n",
    "        return predict_proba(input_data)[:, 1]\n",
    "    else:\n",
    "        return model.predict(input_data)\n",
    "\n",
    "# Serializa el resultado de la predicción al correspondiente content type deseado\n",
    "def output_fn(predictions, response_content_type):\n",
    "    if response_content_type == 'application/json':        \n",
    "        return json.dumps(predictions.tolist())\n",
    "    elif response_content_type == 'text/csv':\n",
    "        predictions_response = io.StringIO()\n",
    "        np.savetxt(predictions_response, predictions, delimiter=',')\n",
    "        return predictions_response.getvalue()\n",
    "    else:\n",
    "        raise ValueError(\"El endpoint del modelo solamente soporta Content-Types: 'application/json' o 'text/csv' como respuesta\")        \n",
    "        \n",
    "def read_pkl(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def to_pkl(data, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def random_forest(**hyperparameters):\n",
    "    return RandomForestClassifier(n_jobs=-1, \n",
    "                                  min_samples_split=hyperparameters['min_samples_split'],\n",
    "                                  n_estimators=hyperparameters['n_estimators'],\n",
    "                                  max_depth=hyperparameters['max_depth'],\n",
    "                                  max_features=hyperparameters['max_features'])\n",
    "\n",
    "def gradient_boosting(**hyperparameters):\n",
    "    return GradientBoostingClassifier(learning_rate=hyperparameters['learning_rate'],\n",
    "                                      min_samples_split=hyperparameters['min_samples_split'],\n",
    "                                      n_estimators=hyperparameters['n_estimators'],\n",
    "                                      max_depth=hyperparameters['max_depth'],\n",
    "                                      max_features=hyperparameters['max_features'])\n",
    "\n",
    "def extra_trees(**hyperparameters):\n",
    "    return ExtraTreesClassifier(n_jobs=-1, \n",
    "                                min_samples_split=hyperparameters['min_samples_split'],\n",
    "                                n_estimators=hyperparameters['n_estimators'],\n",
    "                                max_depth=hyperparameters['max_depth'],\n",
    "                                max_features=hyperparameters['max_features'])\n",
    "\n",
    "def invalid_algorithm(**hyperparameters):\n",
    "    raise Exception('Invalid Algorithm')\n",
    "    \n",
    "def algorithm_selector(algorithm, **hyperparameters):\n",
    "    algorithms = {\n",
    "        'RandomForest': random_forest,\n",
    "        'GradientBoosting': gradient_boosting,\n",
    "        'ExtraTrees': extra_trees\n",
    "    }\n",
    "    \n",
    "    clf = algorithms.get(algorithm, invalid_algorithm)    \n",
    "    return clf(**hyperparameters)\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    print(f'INFO: {script_name}: Iniciando entrenamiento del modelo')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train-data', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_DATA'))\n",
    "    parser.add_argument('--train-target', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_TARGET'))\n",
    "    \n",
    "    parser.add_argument('--algorithm', type=str)\n",
    "    parser.add_argument('--splits', type=int, default=10)\n",
    "    parser.add_argument('--target-metric', type=str)\n",
    "    \n",
    "    parser.add_argument('--learning-rate', type=float)\n",
    "    parser.add_argument('--min-samples-split', type=int)\n",
    "    parser.add_argument('--n-estimators', type=int)\n",
    "    parser.add_argument('--max-depth', type=int)\n",
    "    parser.add_argument('--max-features', type=int)\n",
    "    \n",
    "            \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print(f'INFO: {script_name}: Parametros recibidos: {args}')\n",
    "\n",
    "    # Cargar datasets\n",
    "    files = os.listdir(args.train_data)\n",
    "    if len(files) == 1:\n",
    "        train_data = pd.read_csv(os.path.join(args.train_data, files[0]))\n",
    "    else:\n",
    "        raise Exception()\n",
    "    \n",
    "    files = os.listdir(args.train_target)\n",
    "    if len(files) == 1:\n",
    "        train_target = pd.read_csv(os.path.join(args.train_target, files[0]))\n",
    "        train_target = train_target['dataset_credit_risk'].tolist()\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "     \n",
    "    clf = algorithm_selector(args.algorithm, \n",
    "                             learning_rate=args.learning_rate,\n",
    "                             min_samples_split=args.min_samples_split,\n",
    "                             n_estimators=args.n_estimators,\n",
    "                             max_depth=args.max_depth,\n",
    "                             max_features=args.max_features)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=args.splits)    \n",
    "    cv_scores = cross_validate(clf, train_data, train_target, cv=skf, scoring=args.target_metric, n_jobs=-1)\n",
    "    print('{} = {}%'.format(args.target_metric, cv_scores['test_score'].mean().round(4)*100))\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    clf.fit(train_data, train_target) \n",
    "    \n",
    "    # Guardar modelo\n",
    "    to_pkl(clf, os.path.join(args.model_dir, 'model.pkl'))\n",
    "\n",
    "    print(f'INFO: {script_name}: Finalizando el entrenamiento del modelo')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empaquetamos el script en un archvio .tar.gz y lo subimos a un bucket de Amazon S3 para poder utilizarlo en el job de entrenamiento y posteriormente para el despliegue del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script_tar_file = os.path.join('code',os.path.splitext(os.path.basename(training_script_file))[0] + '.tar.gz')\n",
    "\n",
    "sagemaker_utils.create_tar_gz(training_script_file, training_script_tar_file)\n",
    "\n",
    "training_script_path = sagemaker_utils.upload(training_script_tar_file, f's3://{bucket}/{code_prefix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función model_fn tiene la lógica para cargar el modelo en memoria y se ejecuta solamente al iniciar por primera vez el contenedor, una vez hecho el despligue del modelo.\n",
    "Con cada petición al endpoint del modelo, una vez desplegado, se ejecutan las siguientes funciones en este respectivo orden:\n",
    "input_fn la cual recibe los datos provenientes de la petición y se encarga de realizar la transformación que sea necesaria para poder posteriormente pasar los datos al modelo para generar la predicción\n",
    "predict_fn se encarga de generar la predicción, recibiendo como entrada la salida de la función input_fn\n",
    "output_fn una vez obtenida la predicción, esta función se encarga de realizar las transformaciones necesarias para devolver la respuesta al solicitante de la petición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear Training Job\n",
    "\n",
    "\n",
    "Para crear el Job de entrenamiento en Amazon SageMaker utilizamos la clase CustomEstimator para crear un Estimator el cual permita integrar nuestro script train_and_serve.py con el contenedor Docker que previamente creamos para el entrenamiento de nuestros modelos.\n",
    "\n",
    "También utilizaremos la expresión regular recall = (\\d+\\.\\d{1,2})? para definir una métrica de desempeño de nuestro algoritmo, en este caso por tratarse de un problema de riesgo, lo que buscamos es incrementar el Recall, esta métrica es calculada mediante el uso de k-Fold Cross-Validation. Amazon SageMaker aplica esta expresión regular a los mensajes de la salida estándar. Posteriormente mediante el uso de esta métrica podremos crear un Job de optimización de hiperparámetros.\n",
    "\n",
    "\n",
    "Para ejecutar los procesos de entrenamiento lo haremos llamando el método fit del estimator previamente creado. Y lo haremos creando tres procesos de entrenamiento en paralelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'GradientBoosting':{}, 'RandomForest':{}, 'ExtraTrees':{}}\n",
    "metric_name = 'cross-val:recall'\n",
    "metric_regex = 'recall = (\\d+\\.\\d{1,2})?'\n",
    "\n",
    "for algorithm in estimators:   \n",
    "    estimators[algorithm] = Estimator(\n",
    "        image_uri = docker_images['Training']['image_uri'],        \n",
    "        entry_point = os.path.basename(training_script_file),\n",
    "        source_dir = training_script_path,\n",
    "        role = sagemaker_role,\n",
    "        instance_count = 1,\n",
    "        instance_type = 'ml.m5.xlarge',\n",
    "        output_path = f's3://{bucket}/{model_prefix}',\n",
    "        metric_definitions = [{'Name': metric_name, 'Regex': metric_regex}],\n",
    "        volume_size = 5,\n",
    "        max_run = 60*60*2, # dos horas\n",
    "        hyperparameters={\n",
    "            'algorithm':algorithm,\n",
    "            'splits':5,\n",
    "            'target-metric':'recall',\n",
    "            'learning-rate': 0.1, \n",
    "            'min-samples-split': 3, \n",
    "            'n-estimators': 300,\n",
    "            'max-depth': 25,\n",
    "            'max-features':20})\n",
    "    \n",
    "    estimators[algorithm].fit(\n",
    "        {'train_data': sagemaker_utils.get_processor_output_path(processor, 'train_data'),\n",
    "        'train_target': sagemaker_utils.get_processor_output_path(processor, 'train_target')},\n",
    "        wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definimos un waiter\n",
    "sagemaker_utils.wait_for_training_jobs(estimators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para obtener las métricas de desempeño de cada estimator, podemos iterar a través de estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in estimators:\n",
    "    metrics = estimators[estimator].training_job_analytics.dataframe()\n",
    "    test_recall = metrics[metrics['metric_name'] == metric_name]['value'].values[0]\n",
    "    print(f'{estimator}: cross-val:recall = {test_recall}%')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
