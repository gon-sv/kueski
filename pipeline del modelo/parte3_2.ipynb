{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear Processing Job:\n",
    "\n",
    "Utilizaremos uno de los contenedores previamente creados para correr nuestro script que acabamos de crear, para esto utilizamos la clase Processor. Y debemos especificar los recursos requeridos para la instancia en dónde se ejecutará el proceso, así cómo la imagen Docker a utilizar y la ubicación del script Python a ejecutar con la lógica para la preparación del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(data_prep_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "definimos las siguientes variables para mas adelante poder re-utilizarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = 'train_data.csv'\n",
    "train_target_file = 'train_target.csv'\n",
    "test_data_file = 'test_data.csv'\n",
    "test_target_file = 'test_target.csv'\n",
    "encoder_file = 'encoder.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ejecutamos el Job utilizando el metodo run del objeto creado mediante la clase Processor. Debemos pasar las rutas de los buckets de Amazon S3 tanto para inputs como para outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_parameters = {\n",
    "    'inputs':[ProcessingInput(input_name='input',\n",
    "                    source=f's3://{bucket}/{datasets_prefix}',\n",
    "                    destination='/opt/ml/processing/input'),\n",
    "              ProcessingInput(input_name='code',\n",
    "                    source=data_prep_script_path,\n",
    "                    destination='/opt/ml/processing/input/code')],\n",
    "    'outputs':[ProcessingOutput(output_name='train_data',\n",
    "                    source=f'/opt/ml/processing/output/train_data',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/train_data'),\n",
    "               ProcessingOutput(output_name='train_target',\n",
    "                    source=f'/opt/ml/processing/output/train_target',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/train_target'),\n",
    "               ProcessingOutput(output_name='test_data',\n",
    "                    source=f'/opt/ml/processing/output/test_data',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/test_data'),\n",
    "               ProcessingOutput(output_name='test_target',\n",
    "                    source=f'/opt/ml/processing/output/test_target',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/test_target'),\n",
    "               ProcessingOutput(output_name='encoder',\n",
    "                    source=f'/opt/ml/processing/output/encoder',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/encoder')],\n",
    "    'arguments':['--test-size', '0.1',\n",
    "                 '--data-file', 'dataset_credit_risk',\n",
    "                 '--train-data-file', train_data_file,\n",
    "                 '--train-target-file', train_target_file,\n",
    "                 '--test-data-file', test_data_file,\n",
    "                 '--test-target-file', test_target_file,\n",
    "                 '--encoder-file', encoder_file]}\n",
    "\n",
    "processor.run(**data_prep_parameters)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
