{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_file = 'data_prep.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_utils.make_dirs(data_prep_script_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $data_prep_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def to_pkl(data, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    \n",
    "    print(f'INFO: {script_name}: Iniciando la preparaci칩n de los datos')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test-size', type=float, default=0.1)\n",
    "    parser.add_argument('--data-file', type=str, default='train.csv')\n",
    "    parser.add_argument('--train-data-file', type=str)\n",
    "    parser.add_argument('--train-target-file', type=str)\n",
    "    parser.add_argument('--test-data-file', type=str)\n",
    "    parser.add_argument('--test-target-file', type=str)\n",
    "    parser.add_argument('--encoder-file', type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    print(f'INFO: {script_name}: Par치metros recibidos: {args}')\n",
    "    \n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    data_path = os.path.join(input_path, args.data_file) \n",
    "    \n",
    "    \n",
    "    # Cargar dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    df = df.sort_values(by=[\"id\", \"loan_date\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"loan_date\"] = pd.to_datetime(df.loan_date)\n",
    "\n",
    "    #Feature nb_previous_loans\n",
    "    df_grouped = df.groupby(\"id\")\n",
    "    df[\"nb_previous_loans\"] = df_grouped[\"loan_date\"].rank(method=\"first\") - 1\n",
    "\n",
    "    # Feature avg_amount_loans_previous\n",
    "    df['avg_amount_loans_previous'] = (df.groupby('id')['loan_amount'].apply(lambda x: x.shift().expanding().mean()))\n",
    "\n",
    "    # Feature age\n",
    "    from datetime import datetime, date\n",
    "\n",
    "    df['birthday'] = pd.to_datetime(df['birthday'], errors='coerce')\n",
    "    df['age'] = (pd.to_datetime('today').normalize() - df['birthday']).dt.days // 365\n",
    "\n",
    "    # Feature years_on_the_job\n",
    "\n",
    "    df['job_start_date'] = pd.to_datetime(df['job_start_date'], errors='coerce')\n",
    "    df['years_on_the_job'] = (pd.to_datetime('today').normalize() - df['job_start_date']).dt.days // 365\n",
    "\n",
    "    # Feature flag_own_car\n",
    "\n",
    "    df['flag_own_car'] = df.flag_own_car.apply(lambda x : 0 if x == 'N' else 1)\n",
    "\n",
    "    # Selecci칩n de columnas\n",
    "    columns = ['id', 'age', 'years_on_the_job', 'nb_previous_loans', 'avg_amount_loans_previous', 'flag_own_car', 'status']\n",
    "    \n",
    "    df = df[columns]\n",
    "\n",
    "    cust_df = df.copy()\n",
    "    cust_df.fillna(0, inplace=True)\n",
    "\n",
    "    Y = cust_df['status'].astype('int')\n",
    "\n",
    "    cust_df.drop(['status'], axis=1, inplace=True)\n",
    "    cust_df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    X = cust_df\n",
    "        \n",
    "    import pandas as pd\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, confusion_matrix, recall_score, \n",
    "        plot_confusion_matrix, precision_score, plot_roc_curve\n",
    "    )\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state = 123)\n",
    "\n",
    "    # Using Synthetic Minority Over-Sampling Technique(SMOTE) to overcome sample imbalance problem.\n",
    "    X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "    X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "    # Guardar los dataframes resultantes y el encoder\n",
    "    X_train.to_csv(os.path.join(output_path, 'train_data', args.train_data_file), index=False)\n",
    "    y_train.to_csv(os.path.join(output_path, 'train_target', args.train_target_file), index=False)\n",
    "    X_test.to_csv(os.path.join(output_path, 'test_data', args.test_data_file), index=False)\n",
    "    y_test.to_csv(os.path.join(output_path, 'test_target', args.test_target_file), index=False)\n",
    "    to_pkl(encoder, os.path.join(output_path, 'encoder', args.encoder_file))\n",
    "\n",
    "    print(f'INFO: {script_name}: Finalizando la preparaci칩n de los datos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y subimos el script creado a un bucket de Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_path = sagemaker_utils.upload(data_prep_script_file, f's3://{bucket}/{code_prefix}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
