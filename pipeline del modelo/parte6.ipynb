{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación de desempeño\n",
    "\n",
    "utilizaremos un Processing Job de Amazon SageMaker pero en este caso para buscar el umbral de clasificación\n",
    "\n",
    "Esto lo haremos buscando maximizar la métrica Recall pero manteniendo un mínimo valor para la métrica Precision, que recibiremos como parámetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models_script_file = 'code/evaluate_models.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $evaluate_models_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def load_model(file, model_file='model.pkl'):\n",
    "    if file.endswith('tar.gz'):\n",
    "        with tarfile.open(file, 'r:gz') as tar:\n",
    "            for name in tar.getnames():\n",
    "                if name == model_file:\n",
    "                    f = tar.extractfile(name)\n",
    "                    return pickle.load(f)\n",
    "            return None\n",
    "    elif file.endswith('pkl'):\n",
    "        with open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    print(f'INFO: {script_name}: Iniciando la evaluación de los modelos')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--algos', type=str, required=True)\n",
    "    parser.add_argument('--min-precision', type=float, required=True)    \n",
    "    parser.add_argument('--test-data-file', type=str, required=True)\n",
    "    parser.add_argument('--test-target-file', type=str, required=True)\n",
    "    parser.add_argument('--thresholds-file', type=str, required=True)   \n",
    "    parser.add_argument('--metrics-report-file', type=str, required=True)    \n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    print(f'INFO: {script_name}: Parámetros recibidos: {args}')\n",
    "    \n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    # Cargar datasets\n",
    "    test_target_path = os.path.join(input_path, 'target', args.test_target_file)     \n",
    "    test_target = pd.read_csv(test_target_path)\n",
    "    \n",
    "    test_data_path = os.path.join(input_path, 'data', args.test_data_file)     \n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Umbrales de decision por algoritmo\n",
    "    algo_metrics = {'Algorithm':[], 'Threshold':[], 'Precision':[], 'Recall':[]}\n",
    "    \n",
    "    metrics_report = {}\n",
    "    \n",
    "    algos = args.algos.split(',')\n",
    "    for algo in algos:\n",
    "        model_path = os.path.join(input_path, algo, 'model.tar.gz')         \n",
    "\n",
    "        # Carga modelo en memoria\n",
    "        print(f'Cargando modelo: {model_path}')\n",
    "        clf = load_model(model_path)\n",
    "        \n",
    "        # Obtiene predicciones con dataset para pruebas\n",
    "        predictions = clf.predict_proba(test_data)[:, 1]\n",
    "        \n",
    "        # Busca umbral de decision\n",
    "        precision, recall, thresholds = precision_recall_curve(test_target, predictions)\n",
    "        operating_point_idx = np.argmax(precision>=args.min_precision)\n",
    "        \n",
    "        algo_metrics['Threshold'].append(thresholds[operating_point_idx])\n",
    "        algo_metrics['Precision'].append(precision[operating_point_idx])\n",
    "        algo_metrics['Recall'].append(recall[operating_point_idx])\n",
    "        algo_metrics['Algorithm'].append(algo)\n",
    "        \n",
    "        metrics_report[algo] = {\n",
    "            'precision': {'value': precision[operating_point_idx], 'standard_deviation': 'NaN'},\n",
    "            'recall': {'value': recall[operating_point_idx], 'standard_deviation': 'NaN'}}\n",
    "         \n",
    "    \n",
    "    # Guardar Thresholds    \n",
    "    metrics = pd.DataFrame(algo_metrics)\n",
    "    print(f'INFO: {script_name}: Thresholds encontrados')\n",
    "    print(metrics)\n",
    "    metrics.to_csv(os.path.join(output_path, args.thresholds_file), index=False)    \n",
    "    \n",
    "    # Guardar reporte de metricas para cada modelo\n",
    "    for algo in metrics_report:\n",
    "        with open(os.path.join(output_path, f'{algo}_metrics.json'), 'w') as f:\n",
    "            json.dump({'binary_classification_metrics':metrics_report[algo]},f)        \n",
    "\n",
    "    \n",
    "    print(f'INFO: {script_name}: Finalizando la evaluación de los modelos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subimos el script a un bucket de Amazon S3 para poder utilizarlo con el Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models_script_path = sagemaker_utils.upload(evaluate_models_script_file, f's3://{bucket}/{code_prefix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que tenemos el script listo en S3, podemos pasar a crear el Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(evaluate_models_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ejecutamos el proceso con el método run(), proporcionando los siguientes parámetros:\n",
    "\n",
    "inputs – rutas de las ubicaciones en Amazon S3 de los archivos de entrada a ser utilizados, en este caso el dataset para pruebas, así como los archivos de predicciones de cada uno de los modelos y el programa Python a ejecutar dentro del contenedor\n",
    "outputs – ruta para guardar el resultado del proceso, en este caso los umbrales para cada modelo\n",
    "arguments – parámetros del proceso que son pasados cómo argumentos de la línea de comandos al programa Python que se ejecuta dentro del contenedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_file = 'thresholds.csv'\n",
    "metrics_report_file = 'metrics_report.json'\n",
    "\n",
    "eval_parameters = {\n",
    "    'inputs':[ProcessingInput(\n",
    "                  input_name='code',\n",
    "                  source=evaluate_models_script_path,\n",
    "                  destination='/opt/ml/processing/input/code'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_processor_output_path(processor, 'test_target'), \n",
    "                  destination='/opt/ml/processing/input/target'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_processor_output_path(processor, 'test_data'), \n",
    "                  destination='/opt/ml/processing/input/data'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_tuner_best_model_artifacts_path(tuners['GradientBoosting']), \n",
    "                  destination='/opt/ml/processing/input/GradientBoosting'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_tuner_best_model_artifacts_path(tuners['RandomForest']),\n",
    "                  destination='/opt/ml/processing/input/RandomForest'),\n",
    "              ProcessingInput(\n",
    "                  source=sagemaker_utils.get_tuner_best_model_artifacts_path(tuners['ExtraTrees']), \n",
    "                  destination='/opt/ml/processing/input/ExtraTrees')],\n",
    "    'outputs':[ProcessingOutput(\n",
    "                   output_name='eval',\n",
    "                   source='/opt/ml/processing/output',\n",
    "                   destination=f's3://{bucket}/{eval_prefix}')],\n",
    "    'arguments':['--algos', ','.join(estimators.keys()),\n",
    "                 '--min-precision', '0.85',\n",
    "                 '--test-data-file', test_data_file,\n",
    "                 '--test-target-file', test_target_file,\n",
    "                 '--thresholds-file', thresholds_file,\n",
    "                 '--metrics-report-file', metrics_report_file]}\n",
    "\n",
    "evaluation_processor.run(**eval_parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_path = sagemaker_utils.get_processor_output_path(evaluation_processor,'eval')\n",
    "metrics = sagemaker_utils.read_csv(f'{thresholds_path}/{thresholds_file}')\n",
    "\n",
    "max_recall = metrics[metrics['Recall']==metrics['Recall'].max()]\n",
    "best_model_found = max_recall.loc[max_recall['Precision'].idxmax()]\n",
    "\n",
    "best_model_found\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f087cbbaaafa03b885b484966f26ebaef7143783b0ae783847677076d63b434"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('drift_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
